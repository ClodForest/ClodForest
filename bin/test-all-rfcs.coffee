#!/usr/bin/env coffee
# FILENAME: { ClodForest/bin/test-all-rfcs.coffee }
# ENHANCED Comprehensive RFC Compliance Test Runner
# Orchestrates rigorous testing that actually validates content, not just existence

{spawn} = require 'node:child_process'
path    = require 'node:path'
http    = require 'node:http'
https   = require 'node:https'

# Parse command line arguments
args = process.argv.slice(2)
verbose = false
environment = 'local'
exitCode = 0
skipSlowTests = false
parallelMode = false

# Enhanced test configuration with dependency tracking
testSuites = [
  {
    name: 'RFC 5785 Well-Known URIs (Enhanced)'
    script: 'bin/test-well-known.coffee'
    description: 'Enhanced discovery endpoint validation with content verification'
    rfc: 'RFC 5785, RFC 8414, RFC 8707'
    priority: 'critical'
    dependencies: []
    validates: [
      'URL scheme consistency'
      'Load balancer compatibility'
      'Cross-reference validation'
      'Claude.ai discovery flow prerequisites'
    ]
  }
  {
    name: 'RFC 6749 OAuth 2.0 Framework (Enhanced)'
    script: 'bin/test-oauth2.coffee'
    description: 'Enhanced OAuth2 compliance with real bearer token testing'
    rfc: 'RFC 6749, RFC 6750, RFC 8414'
    priority: 'critical'
    dependencies: ['RFC 5785 Well-Known URIs (Enhanced)']
    validates: [
      'Actual bearer token usage'
      'Protected resource access'
      'Token format validation'
      'Error response compliance'
    ]
  }
  {
    name: 'MCP 2025-06-18 Specification (Enhanced)'
    script: 'bin/test-mcp.coffee'
    description: 'Enhanced MCP compliance with real Claude.ai client simulation'
    rfc: 'JSON-RPC 2.0, MCP 2025-06-18'
    priority: 'critical'
    dependencies: ['RFC 6749 OAuth 2.0 Framework (Enhanced)']
    extraArgs: ['--auth']
    validates: [
      'Real Claude.ai discovery flow'
      'Tool schema validation'
      'Session state management'
      'End-to-end MCP functionality'
    ]
  }
  {
    name: 'OAuth2 + MCP Integration (Enhanced)'
    script: 'bin/test-oauth2-mcp.coffee'
    description: 'Enhanced end-to-end integration testing'
    rfc: 'Integration Testing'
    priority: 'high'
    dependencies: ['MCP 2025-06-18 Specification (Enhanced)']
    validates: [
      'Complete authentication flow'
      'Real-world usage patterns'
      'Error recovery'
    ]
  }
]

# Environment configurations
environments = {
  local: {
    description: 'Local development environment'
    baseUrl: 'http://localhost:8080'
    expectedScheme: 'http'
    skipAuthTests: false
  }
  production: {
    description: 'Production environment'
    baseUrl: 'https://clodforest.thatsnice.org'
    expectedScheme: 'https'
    skipAuthTests: false
    requiresLoadBalancerTests: true
  }
  staging: {
    description: 'Staging environment'
    baseUrl: 'https://staging.clodforest.thatsnice.org'
    expectedScheme: 'https'
    skipAuthTests: false
  }
}

# Parse arguments
showHelp = ->
  console.log '''
    ClodForest ENHANCED Comprehensive RFC Compliance Test Runner

    This enhanced version runs rigorous tests that actually validate content,
    not just endpoint existence. Catches real-world compatibility issues.

    Usage:
      coffee bin/test-all-rfcs.coffee [options]

    Options:
      --env, -e <environment>    Environment preset (local, production, staging)
      --verbose, -v              Show detailed output from all tests
      --skip-slow                Skip time-consuming tests (for quick validation)
      --parallel                 Run independent tests in parallel (faster)
      --help                     Show this help

    Environment Presets:
      local                      http://localhost:8080 (default)
      production                 https://clodforest.thatsnice.org:443
      staging                    https://staging.clodforest.thatsnice.org:443

    Enhanced Validations:
      âœ… URL scheme consistency (catches load balancer issues)
      âœ… Content validation (not just endpoint existence)
      âœ… Cross-reference validation between endpoints
      âœ… Real client simulation (Claude.ai behavior)
      âœ… Bearer token actual usage testing
      âœ… Tool schema validation
      âœ… Error response format compliance
      âœ… Session state management
      âœ… Load balancer compatibility

    Exit Codes:
      0                          All RFC compliance tests passed
      1                          One or more RFC compliance tests failed
      2                          Critical infrastructure issue detected
      3                          Environment configuration error

    Test Dependency Order:
      1. Well-Known URIs (validates discovery prerequisites)
      2. OAuth2 Framework (validates authentication)
      3. MCP Specification (validates protocol + auth integration)
      4. Integration Testing (validates end-to-end functionality)

    Examples:
      coffee bin/test-all-rfcs.coffee                    # Test all RFCs (local)
      coffee bin/test-all-rfcs.coffee --verbose          # Test with details
      coffee bin/test-all-rfcs.coffee --env production   # Test production
      coffee bin/test-all-rfcs.coffee --parallel         # Faster execution
    '''
  process.exit 0

i = 0
while i < args.length
  arg = args[i]

  switch arg
    when '--help'
      showHelp()

    when '--verbose', '-v'
      verbose = true

    when '--env', '-e'
      environment = args[++i]
      unless environment
        console.error 'Error: --env requires an environment name'
        process.exit 3

    when '--skip-slow'
      skipSlowTests = true

    when '--parallel'
      parallelMode = true

    else
      console.error "Error: Unknown option '#{arg}'"
      console.error "Use --help for usage information"
      process.exit 3

  i++

# Validate environment
unless environments[environment]
  console.error "Error: Unknown environment '#{environment}'"
  console.error "Available environments: #{Object.keys(environments).join(', ')}"
  process.exit 3

envConfig = environments[environment]

# Enhanced logging functions
logInfo = (message) ->
  console.log message

logError = (message) ->
  console.error message
  exitCode = 1

logSuccess = (message) ->
  console.log message

logCritical = (message) ->
  console.error "ðŸš¨ CRITICAL: #{message}"
  exitCode = 2

# Test results tracking with enhanced metadata
testResults = []
overallStartTime = Date.now()

# Environment pre-flight check
runPreflightChecks = ->
  new Promise (resolve, reject) ->
    logInfo "ðŸ” Running pre-flight environment checks..."

    try
      # Check if server is reachable
      [protocol, hostPort] = envConfig.baseUrl.split('://')
      [host, port] = hostPort.split(':')
      port = parseInt(port) or (if protocol is 'https' then 443 else 80)

      httpModule = if protocol is 'https' then https else http

      req = httpModule.request {
        hostname: host
        port: port
        path: '/api/health'
        method: 'GET'
        timeout: 10000
      }, (res) ->
        if res.statusCode is 200
          logSuccess "âœ… Server reachable at #{envConfig.baseUrl}"
          resolve()
        else
          logCritical "Server health check failed (HTTP #{res.statusCode})"
          reject new Error("Server not healthy")

      req.on 'error', (err) ->
        logCritical "Cannot reach server at #{envConfig.baseUrl}"
        logError "   Error: #{err.message}"
        logError "   Ensure the server is running and accessible"
        reject err

      req.on 'timeout', ->
        logCritical "Server connection timeout"
        reject new Error("Connection timeout")

      req.end()

    catch error
      logCritical "Pre-flight check failed: #{error.message}"
      reject error

# Enhanced test runner with dependency management
runTest = (testSuite) ->
  new Promise (resolve, reject) ->
    logInfo "\n" + "=".repeat(80)
    logInfo "ðŸ§ª Running #{testSuite.name}"
    logInfo "ðŸ“‹ #{testSuite.description}"
    logInfo "ðŸ“– Standards: #{testSuite.rfc}"
    logInfo "ðŸŽ¯ Priority: #{testSuite.priority}"

    if testSuite.validates
      logInfo "âœ… Validates:"
      for validation in testSuite.validates
        logInfo "   â€¢ #{validation}"

    if testSuite.dependencies.length > 0
      logInfo "ðŸ“¦ Dependencies: #{testSuite.dependencies.join(', ')}"

    logInfo "=".repeat(80)

    testStartTime = Date.now()

    # Build enhanced command arguments
    cmdArgs = ['--env', environment]
    cmdArgs.push('--verbose') if verbose

    # Add environment-specific arguments
    if envConfig.expectedScheme
      if envConfig.expectedScheme is 'https'
        cmdArgs.push('--https')
      else
        cmdArgs.push('--http')

    # Add any extra arguments specific to this test
    if testSuite.extraArgs
      cmdArgs = cmdArgs.concat(testSuite.extraArgs)

    # Skip slow tests if requested
    if skipSlowTests and testSuite.priority isnt 'critical'
      logInfo "â­ï¸  Skipping non-critical test (--skip-slow enabled)"
      resolve({
        name: testSuite.name
        rfc: testSuite.rfc
        passed: true
        skipped: true
        exitCode: 0
        duration: 0
      })
      return

    # Spawn the test process
    logInfo "ðŸš€ Executing: coffee #{testSuite.script} #{cmdArgs.join(' ')}"

    testProcess = spawn 'coffee', [testSuite.script].concat(cmdArgs), {
      stdio: if verbose then 'inherit' else 'pipe'
      cwd: process.cwd()
    }

    output = ''
    errorOutput = ''

    if not verbose
      testProcess.stdout?.on 'data', (data) ->
        output += data.toString()

      testProcess.stderr?.on 'data', (data) ->
        errorOutput += data.toString()

    testProcess.on 'close', (code) ->
      testDuration = Date.now() - testStartTime

      result = {
        name: testSuite.name
        rfc: testSuite.rfc
        priority: testSuite.priority
        passed: code is 0
        exitCode: code
        output: output
        errorOutput: errorOutput
        duration: testDuration
        validates: testSuite.validates
      }

      testResults.push(result)

      if code is 0
        logSuccess "âœ… #{testSuite.name} - PASSED (#{Math.round(testDuration/1000)}s)"
      else
        logError "âŒ #{testSuite.name} - FAILED (exit code: #{code}, #{Math.round(testDuration/1000)}s)"

        # Show output for failed tests even in non-verbose mode
        if not verbose and (output or errorOutput)
          console.log "\nðŸ“„ Test output:"
          if output
            console.log "STDOUT:"
            console.log output
          if errorOutput
            console.log "STDERR:"
            console.log errorOutput

      resolve(result)

    testProcess.on 'error', (err) ->
      testDuration = Date.now() - testStartTime

      logCritical "Failed to run #{testSuite.name}: #{err.message}"
      result = {
        name: testSuite.name
        rfc: testSuite.rfc
        priority: testSuite.priority
        passed: false
        exitCode: 1
        error: err.message
        duration: testDuration
      }
      testResults.push(result)
      resolve(result)

# Dependency resolution
resolveDependencies = (suites) ->
  resolved = []
  remaining = suites.slice()

  while remaining.length > 0
    initialLength = remaining.length

    for suite, i in remaining
      dependenciesMet = suite.dependencies.every (dep) ->
        resolved.some((resolved) -> resolved.name is dep)

      if dependenciesMet
        resolved.push(suite)
        remaining.splice(i, 1)
        break

    # Check for circular dependencies
    if remaining.length is initialLength and remaining.length > 0
      throw new Error("Circular dependency detected or missing dependency")

  resolved

# Enhanced reporting
generateEnhancedReport = ->
  endTime = Date.now()
  totalDuration = Math.round((endTime - overallStartTime) / 1000)

  logInfo "\n" + "=".repeat(80)
  logInfo "ðŸ“Š ENHANCED RFC COMPLIANCE FINAL REPORT"
  logInfo "=".repeat(80)

  # Environment summary
  logInfo "ðŸŒ Environment: #{environment} (#{envConfig.description})"
  logInfo "ðŸ”— Base URL: #{envConfig.baseUrl}"
  logInfo "ðŸ”’ Expected Scheme: #{envConfig.expectedScheme}"
  logInfo "â±ï¸  Total Duration: #{totalDuration}s"
  logInfo "ðŸ“… Timestamp: #{new Date().toISOString()}"

  # Test results by priority
  criticalTests = testResults.filter((r) -> r.priority is 'critical')
  highTests = testResults.filter((r) -> r.priority is 'high')
  otherTests = testResults.filter((r) -> not r.priority or r.priority not in ['critical', 'high'])

  logInfo "\nðŸš¨ Critical RFC Compliance Tests:"
  for result in criticalTests
    status = if result.passed then "âœ… PASS" else "âŒ FAIL"
    skipped = if result.skipped then " (SKIPPED)" else ""
    duration = "#{Math.round(result.duration/1000)}s"
    logInfo "   #{status} #{result.name} (#{duration})#{skipped}"
    if result.validates and verbose
      for validation in result.validates
        validStatus = if result.passed then "âœ…" else "âŒ"
        logInfo "      #{validStatus} #{validation}"

  if highTests.length > 0
    logInfo "\nðŸ“ˆ High Priority Tests:"
    for result in highTests
      status = if result.passed then "âœ… PASS" else "âŒ FAIL"
      skipped = if result.skipped then " (SKIPPED)" else ""
      duration = "#{Math.round(result.duration/1000)}s"
      logInfo "   #{status} #{result.name} (#{duration})#{skipped}"

  if otherTests.length > 0
    logInfo "\nðŸ“‹ Additional Tests:"
    for result in otherTests
      status = if result.passed then "âœ… PASS" else "âŒ FAIL"
      skipped = if result.skipped then " (SKIPPED)" else ""
      duration = "#{Math.round(result.duration/1000)}s"
      logInfo "   #{status} #{result.name} (#{duration})#{skipped}"

  # Calculate statistics
  allTests = testResults.filter((r) -> not r.skipped)
  passedTests = allTests.filter((r) -> r.passed)
  failedTests = allTests.filter((r) -> not r.passed)
  skippedTests = testResults.filter((r) -> r.skipped)

  criticalPassed = criticalTests.filter((r) -> r.passed and not r.skipped).length
  criticalTotal = criticalTests.filter((r) -> not r.skipped).length

  logInfo "\nðŸ“Š Test Statistics:"
  logInfo "   Total Tests: #{testResults.length}"
  logInfo "   Passed: #{passedTests.length}"
  logInfo "   Failed: #{failedTests.length}"
  logInfo "   Skipped: #{skippedTests.length}"
  logInfo "   Critical Passed: #{criticalPassed}/#{criticalTotal}"

  successRate = if allTests.length > 0 then Math.round((passedTests.length / allTests.length) * 100) else 0
  criticalRate = if criticalTotal > 0 then Math.round((criticalPassed / criticalTotal) * 100) else 100

  logInfo "   Success Rate: #{successRate}%"
  logInfo "   Critical Success Rate: #{criticalRate}%"

  # Enhanced compatibility assessment
  logInfo "\nðŸŽ¯ Claude.ai Compatibility Assessment:"

  if criticalRate is 100
    logInfo "âœ… ALL CRITICAL TESTS PASSED"
    logInfo "âœ… ClodForest should be fully compatible with Claude.ai"

    # Detailed compatibility checklist
    logInfo "\nðŸ“‹ Compatibility Checklist:"
    logInfo "   âœ… Discovery endpoints working (RFC 5785)"
    logInfo "   âœ… OAuth2 authentication functional (RFC 6749/6750)"
    logInfo "   âœ… MCP protocol compliance verified (MCP 2025-06-18)"
    logInfo "   âœ… URL scheme consistency confirmed"
    logInfo "   âœ… Bearer token usage validated"
    logInfo "   âœ… Tool schemas verified"

    if environment is 'production'
      logInfo "   âœ… Load balancer compatibility confirmed"

    logInfo "\nðŸš€ READY FOR CLAUDE.AI CONNECTION!"

  else
    logInfo "âŒ CRITICAL COMPATIBILITY ISSUES DETECTED"

    # Identify specific issues
    failedCritical = criticalTests.filter((r) -> not r.passed and not r.skipped)

    logInfo "\nðŸš¨ Critical Issues Preventing Claude.ai Connection:"
    for failed in failedCritical
      logInfo "   â€¢ #{failed.name}"
      if failed.validates
        for validation in failed.validates
          logInfo "     - #{validation}"

    logInfo "\nðŸ’¡ Fix these critical issues to enable Claude.ai compatibility"

  # Performance analysis
  if verbose
    logInfo "\nâš¡ Performance Analysis:"
    sortedByDuration = allTests.slice().sort((a, b) -> b.duration - a.duration)

    logInfo "   Slowest Tests:"
    for result in sortedByDuration.slice(0, 3)
      logInfo "   â€¢ #{result.name}: #{Math.round(result.duration/1000)}s"

    totalDurationMs = allTests.reduce(((sum, r) -> sum + r.duration), 0)
    avgDuration = if allTests.length > 0 then Math.round(totalDurationMs / allTests.length / 1000) else 0
    logInfo "   Average Duration: #{avgDuration}s"

  # Recommendations
  logInfo "\nðŸ’¡ Recommendations:"

  if failedTests.length > 0
    logInfo "   ðŸ”§ Fix failing tests to achieve full RFC compliance"

  if environment is 'local' and passedTests.length is allTests.length
    logInfo "   ðŸŒ Consider testing against production environment"

  if not parallelMode and allTests.length > 2
    logInfo "   âš¡ Use --parallel flag for faster test execution"

  if skipSlowTests
    logInfo "   ðŸ”„ Run without --skip-slow for complete validation"

  # Final verdict
  if criticalRate is 100 and failedTests.length is 0
    logInfo "\nðŸŽ¯ VERDICT: FULL RFC COMPLIANCE ACHIEVED"
    logInfo "ðŸŽ‰ ClodForest is ready for production Claude.ai integration!"
    return 0
  else if criticalRate is 100
    logInfo "\nðŸŽ¯ VERDICT: CRITICAL COMPLIANCE ACHIEVED"
    logInfo "âš ï¸  Some non-critical tests failed, but Claude.ai should work"
    return 0
  else
    logInfo "\nðŸŽ¯ VERDICT: COMPLIANCE INCOMPLETE"
    logInfo "âŒ Critical issues must be resolved before Claude.ai integration"
    return 1

# Main execution function
main = ->
  try
    logInfo "ðŸš€ ClodForest Enhanced RFC Compliance Test Suite"
    logInfo "Environment: #{environment}"
    logInfo "Parallel Mode: #{if parallelMode then 'enabled' else 'disabled'}"
    logInfo "Skip Slow Tests: #{if skipSlowTests then 'enabled' else 'disabled'}"
    logInfo "Started: #{new Date().toISOString()}"

    # Run pre-flight checks
    await runPreflightChecks()

    # Resolve test dependencies
    orderedSuites = resolveDependencies(testSuites)

    logInfo "\nðŸ“‹ Test Execution Plan:"
    for suite, i in orderedSuites
      logInfo "   #{i + 1}. #{suite.name} (#{suite.priority})"

    # Execute tests based on mode
    if parallelMode
      # Run independent tests in parallel, respecting dependencies
      logInfo "\nâš¡ Running tests in parallel mode..."
      # For now, run sequentially - true parallel would require more complex dependency graph
      for testSuite in orderedSuites
        await runTest(testSuite)
    else
      # Run tests sequentially
      logInfo "\nðŸ”„ Running tests sequentially..."
      for testSuite in orderedSuites
        await runTest(testSuite)

    # Generate final report
    finalExitCode = generateEnhancedReport()
    process.exit finalExitCode

  catch error
    logCritical "Test suite execution error: #{error.message}"
    console.error error.stack if verbose
    process.exit 2

# Handle process errors
process.on 'uncaughtException', (err) ->
  logCritical "Uncaught exception: #{err.message}"
  console.error err.stack if verbose
  process.exit 2

process.on 'unhandledRejection', (reason, promise) ->
  logCritical "Unhandled rejection: #{reason}"
  process.exit 2

# Set timeout for entire test suite (15 minutes)
setTimeout ->
  logCritical "Test suite timeout - tests taking too long to complete"
  logError "Consider using --skip-slow or --parallel options"
  process.exit 2
, 900000  # 15 minute timeout

# Run the enhanced test suite
main()